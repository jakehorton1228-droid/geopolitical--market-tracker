{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01 - Data Exploration (EDA)\n",
    "\n",
    "## What is Exploratory Data Analysis?\n",
    "\n",
    "**Exploratory Data Analysis (EDA)** is the critical first step in any data science project. Before building models or running analyses, you need to understand your data:\n",
    "\n",
    "- **What does the data look like?** (shape, types, distributions)\n",
    "- **Is it clean?** (missing values, outliers, errors)\n",
    "- **What patterns exist?** (correlations, trends, clusters)\n",
    "- **What questions can we answer?** (feasibility check)\n",
    "\n",
    "### Why EDA Matters\n",
    "\n",
    "| Without EDA | With EDA |\n",
    "|-------------|----------|\n",
    "| Build model on bad data | Catch data quality issues early |\n",
    "| Misinterpret results | Understand what's actually happening |\n",
    "| Waste time on wrong approach | Choose appropriate methods |\n",
    "| Miss obvious insights | Find low-hanging fruit |\n",
    "\n",
    "### What We'll Cover\n",
    "\n",
    "1. **Database Connection** - Verify data pipeline works\n",
    "2. **GDELT Events Analysis** - Understand geopolitical event data\n",
    "3. **Market Data Exploration** - Understand financial time series\n",
    "4. **Initial Correlations** - First look at relationships\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports\n",
    "\n",
    "### Best Practice: Organize Imports\n",
    "\n",
    "Professional data scientists organize imports into groups:\n",
    "1. **Standard library** (sys, pathlib, datetime)\n",
    "2. **Third-party packages** (pandas, numpy, matplotlib)\n",
    "3. **Local project imports** (our custom modules)\n",
    "\n",
    "This makes code readable and helps identify dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# STANDARD LIBRARY IMPORTS\n",
    "# =============================================================================\n",
    "# These come with Python - no installation needed\n",
    "\n",
    "import sys\n",
    "from pathlib import Path  # Modern way to handle file paths (better than os.path)\n",
    "from datetime import date, timedelta  # For date arithmetic\n",
    "\n",
    "# =============================================================================\n",
    "# PATH SETUP\n",
    "# =============================================================================\n",
    "# Jupyter notebooks run from the /notebooks directory, but our code is in /src\n",
    "# We need to add the project root to Python's path so imports work\n",
    "\n",
    "project_root = Path.cwd().parent  # Go up one level from /notebooks to project root\n",
    "sys.path.insert(0, str(project_root))  # Add to beginning of path\n",
    "\n",
    "# =============================================================================\n",
    "# THIRD-PARTY IMPORTS\n",
    "# =============================================================================\n",
    "# These need to be installed (pip install pandas numpy matplotlib seaborn)\n",
    "\n",
    "import pandas as pd      # Data manipulation - the backbone of data science in Python\n",
    "import numpy as np       # Numerical computing - fast array operations\n",
    "import matplotlib.pyplot as plt  # Plotting - the classic visualization library\n",
    "import seaborn as sns    # Statistical visualization - prettier plots, built on matplotlib\n",
    "\n",
    "# =============================================================================\n",
    "# VISUALIZATION SETTINGS\n",
    "# =============================================================================\n",
    "# Set consistent style across all plots\n",
    "\n",
    "plt.style.use('seaborn-v0_8-whitegrid')  # Clean, professional look\n",
    "sns.set_palette('husl')  # Colorblind-friendly palette\n",
    "\n",
    "# Magic command: display plots inline in the notebook (not in separate windows)\n",
    "%matplotlib inline\n",
    "\n",
    "# Pandas display options - show more data in outputs\n",
    "pd.set_option('display.max_columns', 20)\n",
    "pd.set_option('display.width', 200)\n",
    "\n",
    "print(\"Setup complete!\")\n",
    "print(f\"Project root: {project_root}\")\n",
    "print(f\"Python version: {sys.version.split()[0]}\")\n",
    "print(f\"Pandas version: {pd.__version__}\")\n",
    "print(f\"NumPy version: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Database Connection & Overview\n",
    "\n",
    "### Why Start Here?\n",
    "\n",
    "Before any analysis, verify that:\n",
    "1. Database is running and accessible\n",
    "2. Data has been ingested successfully\n",
    "3. Tables have reasonable row counts\n",
    "\n",
    "### SQLAlchemy Sessions\n",
    "\n",
    "We use **SQLAlchemy** as our ORM (Object-Relational Mapper). Key concepts:\n",
    "\n",
    "- **Session**: A conversation with the database\n",
    "- **Context Manager** (`with`): Automatically handles opening/closing connections\n",
    "- **Query**: Ask the database for data\n",
    "\n",
    "```python\n",
    "# This pattern ensures the session is properly closed even if errors occur\n",
    "with get_session() as session:\n",
    "    # Do database operations here\n",
    "    result = session.query(Model).all()\n",
    "# Session automatically closed when we exit the 'with' block\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# DATABASE IMPORTS\n",
    "# =============================================================================\n",
    "# Import our database utilities\n",
    "\n",
    "from src.db.connection import get_session  # Session factory\n",
    "from src.db.models import Event, MarketData, AnalysisResult  # ORM models\n",
    "\n",
    "# =============================================================================\n",
    "# CHECK DATABASE CONNECTION AND ROW COUNTS\n",
    "# =============================================================================\n",
    "# This is a simple \"health check\" - make sure everything is working\n",
    "\n",
    "try:\n",
    "    with get_session() as session:\n",
    "        # .count() runs SELECT COUNT(*) - very fast even on large tables\n",
    "        event_count = session.query(Event).count()\n",
    "        market_count = session.query(MarketData).count()\n",
    "        analysis_count = session.query(AnalysisResult).count()\n",
    "    \n",
    "    print(\"Database Overview\")\n",
    "    print(\"=\" * 40)\n",
    "    print(f\"Events:           {event_count:>10,}\")\n",
    "    print(f\"Market Data:      {market_count:>10,}\")\n",
    "    print(f\"Analysis Results: {analysis_count:>10,}\")\n",
    "    print()\n",
    "    \n",
    "    # Sanity checks\n",
    "    if event_count == 0:\n",
    "        print(\"WARNING: No events in database!\")\n",
    "        print(\"Run the GDELT ingestion script first.\")\n",
    "    if market_count == 0:\n",
    "        print(\"WARNING: No market data in database!\")\n",
    "        print(\"Run the market data ingestion script first.\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"ERROR: Could not connect to database: {e}\")\n",
    "    print(\"Make sure PostgreSQL is running: docker compose up -d db\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. GDELT Events Analysis\n",
    "\n",
    "### What is GDELT?\n",
    "\n",
    "**GDELT** (Global Database of Events, Language, and Tone) monitors news media worldwide and extracts structured event data. It answers: **\"Who did what to whom, when, and where?\"**\n",
    "\n",
    "### Key Fields in GDELT Data\n",
    "\n",
    "| Field | Description | Example |\n",
    "|-------|-------------|--------|\n",
    "| `goldstein_scale` | Conflict/cooperation score (-10 to +10) | -7.0 (military action) |\n",
    "| `num_mentions` | How many articles mentioned this event | 500 |\n",
    "| `avg_tone` | Sentiment of coverage (-100 to +100) | -3.5 (slightly negative) |\n",
    "| `event_root_code` | CAMEO event type (01-20) | \"19\" (fight) |\n",
    "| `actor1_country` | Country of first actor | \"USA\" |\n",
    "| `actor2_country` | Country of second actor | \"RUS\" |\n",
    "\n",
    "### Understanding Goldstein Scale\n",
    "\n",
    "The **Goldstein Scale** is the most important field for our analysis:\n",
    "\n",
    "```\n",
    "-10 ─────────── 0 ─────────── +10\n",
    " │              │              │\n",
    " │              │              └── Maximum cooperation\n",
    " │              │                  (e.g., military alliance)\n",
    " │              │\n",
    " │              └── Neutral\n",
    " │                  (e.g., meeting, statement)\n",
    " │\n",
    " └── Maximum conflict\n",
    "     (e.g., war, mass violence)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# LOAD EVENTS INTO PANDAS DATAFRAME\n",
    "# =============================================================================\n",
    "# We convert SQLAlchemy objects to a pandas DataFrame for easier analysis\n",
    "#\n",
    "# WHY DATAFRAMES?\n",
    "# - Fast vectorized operations (no slow Python loops)\n",
    "# - Built-in statistics (.mean(), .std(), .describe())\n",
    "# - Easy grouping and aggregation (.groupby())\n",
    "# - Integration with visualization libraries\n",
    "\n",
    "from src.db.queries import get_events_by_date_range\n",
    "\n",
    "# Define date range - let's look at the last 30 days\n",
    "end_date = date.today()\n",
    "start_date = end_date - timedelta(days=30)\n",
    "\n",
    "print(f\"Loading events from {start_date} to {end_date}...\")\n",
    "\n",
    "with get_session() as session:\n",
    "    # Query returns SQLAlchemy objects\n",
    "    events = get_events_by_date_range(session, start_date, end_date)\n",
    "    \n",
    "    # Convert to list of dictionaries, then to DataFrame\n",
    "    # This is a common pattern: ORM objects → dict → DataFrame\n",
    "    events_df = pd.DataFrame([\n",
    "        {\n",
    "            'date': e.event_date,\n",
    "            'goldstein_scale': e.goldstein_scale,\n",
    "            'num_mentions': e.num_mentions,\n",
    "            'num_articles': e.num_articles,\n",
    "            'avg_tone': e.avg_tone,\n",
    "            'event_root_code': e.event_root_code,\n",
    "            'actor1_country': e.actor1_country_code,\n",
    "            'actor2_country': e.actor2_country_code,\n",
    "            'action_country': e.action_geo_country_code,\n",
    "        }\n",
    "        for e in events\n",
    "    ])\n",
    "\n",
    "print(f\"Loaded {len(events_df):,} events\")\n",
    "print(f\"Date range: {events_df['date'].min()} to {events_df['date'].max()}\")\n",
    "print(f\"DataFrame shape: {events_df.shape}\")\n",
    "print()\n",
    "\n",
    "# Show first few rows\n",
    "# .head() is your friend - always look at your data!\n",
    "events_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Descriptive Statistics\n",
    "\n",
    "The `.describe()` method gives you the **five-number summary** plus mean and standard deviation:\n",
    "\n",
    "- **count**: Number of non-null values\n",
    "- **mean**: Average value\n",
    "- **std**: Standard deviation (spread)\n",
    "- **min**: Minimum value\n",
    "- **25%**: First quartile (Q1)\n",
    "- **50%**: Median (Q2)\n",
    "- **75%**: Third quartile (Q3)\n",
    "- **max**: Maximum value\n",
    "\n",
    "**Pro Tip**: Compare mean vs median. If they're very different, you have skewed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# DESCRIPTIVE STATISTICS\n",
    "# =============================================================================\n",
    "# .describe() gives us a quick statistical overview\n",
    "\n",
    "print(\"Event Statistics\")\n",
    "print(\"=\" * 60)\n",
    "print()\n",
    "\n",
    "# Round for readability\n",
    "stats = events_df.describe().round(2)\n",
    "display(stats)\n",
    "\n",
    "print()\n",
    "print(\"KEY OBSERVATIONS:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Calculate some insights\n",
    "if len(events_df) > 0:\n",
    "    avg_goldstein = events_df['goldstein_scale'].mean()\n",
    "    median_goldstein = events_df['goldstein_scale'].median()\n",
    "    \n",
    "    print(f\"• Average Goldstein: {avg_goldstein:.2f} ({'conflict-leaning' if avg_goldstein < 0 else 'cooperation-leaning'})\")\n",
    "    print(f\"• Median Goldstein: {median_goldstein:.2f}\")\n",
    "    \n",
    "    # Skewness check\n",
    "    if abs(avg_goldstein - median_goldstein) > 1:\n",
    "        print(f\"• Data is SKEWED (mean ≠ median)\")\n",
    "    \n",
    "    # Coverage\n",
    "    avg_mentions = events_df['num_mentions'].mean()\n",
    "    max_mentions = events_df['num_mentions'].max()\n",
    "    print(f\"• Average media mentions: {avg_mentions:.0f}\")\n",
    "    print(f\"• Highest media coverage: {max_mentions:,} mentions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Distributions\n",
    "\n",
    "**Histograms** show the distribution of a single variable:\n",
    "- **Shape**: Normal? Skewed? Bimodal?\n",
    "- **Center**: Where's the bulk of data?\n",
    "- **Spread**: How variable is it?\n",
    "- **Outliers**: Are there extreme values?\n",
    "\n",
    "**Why This Matters**: Distribution shape affects which statistical methods you can use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# GOLDSTEIN SCALE DISTRIBUTION\n",
    "# =============================================================================\n",
    "# The Goldstein scale is our primary measure of event severity\n",
    "# Let's understand its distribution\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# LEFT: Histogram of Goldstein scores\n",
    "# -----------------------------------------------------------------------------\n",
    "ax = axes[0]\n",
    "\n",
    "# dropna() removes missing values - histograms can't handle NaN\n",
    "goldstein_values = events_df['goldstein_scale'].dropna()\n",
    "\n",
    "# bins=50 gives us good granularity; edgecolor makes bars distinguishable\n",
    "ax.hist(goldstein_values, bins=50, edgecolor='black', alpha=0.7, color='steelblue')\n",
    "\n",
    "# Reference lines help interpretation\n",
    "ax.axvline(x=0, color='red', linestyle='--', linewidth=2, label='Neutral (0)')\n",
    "ax.axvline(x=goldstein_values.mean(), color='green', linestyle='--', \n",
    "           linewidth=2, label=f'Mean ({goldstein_values.mean():.2f})')\n",
    "\n",
    "ax.set_xlabel('Goldstein Scale', fontsize=12)\n",
    "ax.set_ylabel('Frequency', fontsize=12)\n",
    "ax.set_title('Distribution of Event Severity\\n(-10 = Conflict, +10 = Cooperation)', fontsize=14)\n",
    "ax.legend()\n",
    "\n",
    "# Add text annotation explaining the scale\n",
    "ax.text(-9, ax.get_ylim()[1]*0.9, 'CONFLICT', fontsize=10, color='red', fontweight='bold')\n",
    "ax.text(7, ax.get_ylim()[1]*0.9, 'COOPERATION', fontsize=10, color='green', fontweight='bold')\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# RIGHT: Daily average over time\n",
    "# -----------------------------------------------------------------------------\n",
    "ax = axes[1]\n",
    "\n",
    "# Group by date and calculate mean Goldstein\n",
    "# This shows the \"mood\" of global events each day\n",
    "daily_goldstein = events_df.groupby('date')['goldstein_scale'].mean()\n",
    "\n",
    "ax.plot(daily_goldstein.index, daily_goldstein.values, \n",
    "        marker='o', markersize=4, linewidth=1, color='steelblue')\n",
    "ax.axhline(y=0, color='red', linestyle='--', alpha=0.5, label='Neutral')\n",
    "\n",
    "# Fill above/below zero to show conflict vs cooperation periods\n",
    "ax.fill_between(daily_goldstein.index, daily_goldstein.values, 0,\n",
    "                where=(daily_goldstein.values >= 0), \n",
    "                color='green', alpha=0.3, label='Cooperation days')\n",
    "ax.fill_between(daily_goldstein.index, daily_goldstein.values, 0,\n",
    "                where=(daily_goldstein.values < 0),\n",
    "                color='red', alpha=0.3, label='Conflict days')\n",
    "\n",
    "ax.set_xlabel('Date', fontsize=12)\n",
    "ax.set_ylabel('Average Goldstein Scale', fontsize=12)\n",
    "ax.set_title('Daily Average Event Sentiment Over Time', fontsize=14)\n",
    "ax.tick_params(axis='x', rotation=45)\n",
    "ax.legend(loc='lower left')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print interpretation\n",
    "print(\"\\nINTERPRETATION:\")\n",
    "print(\"-\" * 40)\n",
    "conflict_days = (daily_goldstein < 0).sum()\n",
    "total_days = len(daily_goldstein)\n",
    "print(f\"• {conflict_days}/{total_days} days ({conflict_days/total_days*100:.0f}%) were net conflict\")\n",
    "print(f\"• {total_days - conflict_days}/{total_days} days ({(total_days-conflict_days)/total_days*100:.0f}%) were net cooperation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CAMEO Event Codes\n",
    "\n",
    "GDELT uses **CAMEO** (Conflict and Mediation Event Observations) codes to categorize events:\n",
    "\n",
    "| Code | Category | Examples |\n",
    "|------|----------|----------|\n",
    "| 01-05 | Verbal Cooperation | Statements, appeals, consultations |\n",
    "| 06-09 | Material Cooperation | Aid, yielding, investigations |\n",
    "| 10-14 | Verbal Conflict | Demands, disapproval, threats |\n",
    "| 15-20 | Material Conflict | Force, coercion, assault, war |\n",
    "\n",
    "**Key Insight**: Codes 18-20 (assault, fight, mass violence) are what typically move markets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# EVENT TYPE DISTRIBUTION\n",
    "# =============================================================================\n",
    "# Let's see what types of events dominate our data\n",
    "\n",
    "# CAMEO root code mapping - what each code means\n",
    "event_type_map = {\n",
    "    '01': 'Make Statement',\n",
    "    '02': 'Appeal',\n",
    "    '03': 'Express Cooperation Intent',\n",
    "    '04': 'Consult',\n",
    "    '05': 'Diplomatic Cooperation',\n",
    "    '06': 'Material Cooperation',\n",
    "    '07': 'Provide Aid',\n",
    "    '08': 'Yield',\n",
    "    '09': 'Investigate',\n",
    "    '10': 'Demand',\n",
    "    '11': 'Disapprove',\n",
    "    '12': 'Reject',\n",
    "    '13': 'Threaten',\n",
    "    '14': 'Protest',\n",
    "    '15': 'Exhibit Force',\n",
    "    '16': 'Reduce Relations',\n",
    "    '17': 'Coerce',\n",
    "    '18': 'Assault',\n",
    "    '19': 'Fight',\n",
    "    '20': 'Mass Violence',\n",
    "}\n",
    "\n",
    "# Count events by type\n",
    "# value_counts() is like SQL's GROUP BY + COUNT\n",
    "event_counts = events_df['event_root_code'].value_counts().head(15)\n",
    "\n",
    "# Map codes to readable names\n",
    "event_labels = [event_type_map.get(str(code), str(code)) for code in event_counts.index]\n",
    "\n",
    "# Create horizontal bar chart (easier to read labels)\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "# Color by conflict (red) vs cooperation (green)\n",
    "# Codes 01-09 are cooperation, 10-20 are conflict\n",
    "colors = []\n",
    "for code in event_counts.index:\n",
    "    try:\n",
    "        code_num = int(code)\n",
    "        colors.append('green' if code_num <= 9 else 'orange' if code_num <= 14 else 'red')\n",
    "    except:\n",
    "        colors.append('gray')\n",
    "\n",
    "bars = ax.barh(event_labels, event_counts.values, color=colors, alpha=0.7, edgecolor='black')\n",
    "\n",
    "ax.set_xlabel('Number of Events', fontsize=12)\n",
    "ax.set_title('Top 15 Event Types\\n(Green=Cooperation, Orange=Verbal Conflict, Red=Material Conflict)', fontsize=14)\n",
    "\n",
    "# Add count labels on bars\n",
    "for bar, count in zip(bars, event_counts.values):\n",
    "    ax.text(bar.get_width() + 10, bar.get_y() + bar.get_height()/2,\n",
    "            f'{count:,}', va='center', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Summary statistics\n",
    "print(\"\\nEVENT TYPE SUMMARY:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Calculate percentages by category\n",
    "def categorize_code(code):\n",
    "    try:\n",
    "        c = int(code)\n",
    "        if c <= 9: return 'cooperation'\n",
    "        elif c <= 14: return 'verbal_conflict'\n",
    "        else: return 'material_conflict'\n",
    "    except:\n",
    "        return 'unknown'\n",
    "\n",
    "events_df['event_category'] = events_df['event_root_code'].apply(categorize_code)\n",
    "category_counts = events_df['event_category'].value_counts()\n",
    "\n",
    "for cat, count in category_counts.items():\n",
    "    pct = count / len(events_df) * 100\n",
    "    print(f\"• {cat}: {count:,} events ({pct:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Geographic Analysis\n",
    "\n",
    "Understanding **where** events happen helps us map them to relevant markets:\n",
    "\n",
    "- Events in **Russia** → RUB, natural gas, wheat\n",
    "- Events in **Middle East** → Oil (CL=F)\n",
    "- Events in **China** → FXI ETF, copper, global trade\n",
    "- Events in **USA** → SPY, USD, treasuries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# GEOGRAPHIC DISTRIBUTION\n",
    "# =============================================================================\n",
    "# Which countries generate the most events?\n",
    "\n",
    "country_counts = events_df['action_country'].value_counts().head(15)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "# Color key countries differently\n",
    "key_countries = {'US': 'blue', 'RU': 'red', 'CN': 'orange', 'IR': 'darkred', 'SA': 'green'}\n",
    "colors = [key_countries.get(c, 'steelblue') for c in country_counts.index]\n",
    "\n",
    "bars = ax.bar(country_counts.index, country_counts.values, \n",
    "              color=colors, alpha=0.7, edgecolor='black')\n",
    "\n",
    "ax.set_xlabel('Country Code', fontsize=12)\n",
    "ax.set_ylabel('Number of Events', fontsize=12)\n",
    "ax.set_title('Top 15 Countries by Event Count', fontsize=14)\n",
    "ax.tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Add value labels\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "            f'{int(height):,}', ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nCOUNTRY CODE REFERENCE:\")\n",
    "print(\"-\" * 40)\n",
    "print(\"US=United States, RU=Russia, CN=China, IR=Iran, SA=Saudi Arabia\")\n",
    "print(\"UK=United Kingdom, UA=Ukraine, SY=Syria, IS=Israel, etc.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Market Data Exploration\n",
    "\n",
    "### Financial Time Series Basics\n",
    "\n",
    "Financial data has unique characteristics:\n",
    "\n",
    "1. **Non-stationary prices**: Prices trend over time (can't use simple statistics)\n",
    "2. **Stationary returns**: Price *changes* are more stable (use these for analysis)\n",
    "3. **Volatility clustering**: Big moves follow big moves\n",
    "4. **Fat tails**: Extreme events happen more often than normal distribution predicts\n",
    "\n",
    "### Why We Use Log Returns\n",
    "\n",
    "**Log return** = ln(price_today / price_yesterday)\n",
    "\n",
    "Advantages:\n",
    "- **Additive**: Weekly return = sum of daily returns\n",
    "- **Symmetric**: +10% and -10% are equal magnitude\n",
    "- **More normal**: Better statistical properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# LOAD MARKET DATA\n",
    "# =============================================================================\n",
    "\n",
    "from src.db.queries import get_market_data\n",
    "from src.config.constants import get_all_symbols\n",
    "\n",
    "# See what symbols we track\n",
    "symbols = get_all_symbols()\n",
    "print(f\"Tracking {len(symbols)} symbols:\")\n",
    "print(symbols)\n",
    "print()\n",
    "\n",
    "# Load data for key symbols across different asset classes\n",
    "key_symbols = {\n",
    "    'SPY': 'S&P 500 ETF (US Stocks)',\n",
    "    'CL=F': 'Crude Oil Futures',\n",
    "    'GC=F': 'Gold Futures',\n",
    "    '^VIX': 'Volatility Index (Fear Gauge)',\n",
    "}\n",
    "\n",
    "market_dfs = {}\n",
    "\n",
    "with get_session() as session:\n",
    "    for symbol, description in key_symbols.items():\n",
    "        data = get_market_data(session, symbol, start_date, end_date)\n",
    "        if data:\n",
    "            df = pd.DataFrame([\n",
    "                {\n",
    "                    'date': d.date,\n",
    "                    'close': float(d.close),\n",
    "                    'log_return': d.log_return,\n",
    "                    'volume': d.volume,\n",
    "                }\n",
    "                for d in data\n",
    "            ])\n",
    "            market_dfs[symbol] = df\n",
    "            print(f\"{symbol} ({description}): {len(df)} trading days\")\n",
    "\n",
    "print(f\"\\nLoaded data for {len(market_dfs)} symbols\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# PRICE CHARTS\n",
    "# =============================================================================\n",
    "# Visualize price movements over our analysis period\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, (symbol, description) in enumerate(key_symbols.items()):\n",
    "    if symbol in market_dfs and i < len(axes):\n",
    "        ax = axes[i]\n",
    "        df = market_dfs[symbol]\n",
    "        \n",
    "        # Plot price\n",
    "        ax.plot(df['date'], df['close'], linewidth=2, color='steelblue')\n",
    "        \n",
    "        # Add moving average (shows trend)\n",
    "        if len(df) >= 10:\n",
    "            ma = df['close'].rolling(window=10).mean()\n",
    "            ax.plot(df['date'], ma, linewidth=1, color='red', \n",
    "                    linestyle='--', alpha=0.7, label='10-day MA')\n",
    "        \n",
    "        # Calculate total return for title\n",
    "        total_return = (df['close'].iloc[-1] / df['close'].iloc[0] - 1) * 100\n",
    "        \n",
    "        ax.set_title(f\"{description}\\nTotal Return: {total_return:+.1f}%\", fontsize=12)\n",
    "        ax.set_xlabel('Date')\n",
    "        ax.set_ylabel('Price')\n",
    "        ax.tick_params(axis='x', rotation=45)\n",
    "        ax.legend(loc='upper left')\n",
    "        ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Return Distributions\n",
    "\n",
    "Financial returns have distinctive properties:\n",
    "\n",
    "1. **Centered near zero**: Most days have small moves\n",
    "2. **Fat tails**: More extreme moves than normal distribution predicts\n",
    "3. **Slight negative skew**: Crashes are bigger than rallies (for stocks)\n",
    "4. **Excess kurtosis**: \"Peaked\" distribution with fat tails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# RETURN DISTRIBUTIONS\n",
    "# =============================================================================\n",
    "# Compare return distributions across assets\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, (symbol, description) in enumerate(key_symbols.items()):\n",
    "    if symbol in market_dfs and i < len(axes):\n",
    "        ax = axes[i]\n",
    "        df = market_dfs[symbol]\n",
    "        \n",
    "        # Convert to percentage returns\n",
    "        returns = df['log_return'].dropna() * 100\n",
    "        \n",
    "        if len(returns) > 0:\n",
    "            # Histogram with density (normalized)\n",
    "            ax.hist(returns, bins=30, edgecolor='black', alpha=0.7, \n",
    "                    density=True, color='steelblue', label='Actual')\n",
    "            \n",
    "            # Overlay normal distribution for comparison\n",
    "            from scipy import stats\n",
    "            x = np.linspace(returns.min(), returns.max(), 100)\n",
    "            normal_dist = stats.norm.pdf(x, returns.mean(), returns.std())\n",
    "            ax.plot(x, normal_dist, 'r-', linewidth=2, label='Normal dist.')\n",
    "            \n",
    "            # Reference lines\n",
    "            ax.axvline(x=0, color='black', linestyle='-', alpha=0.5)\n",
    "            ax.axvline(x=returns.mean(), color='green', linestyle='--', \n",
    "                       label=f'Mean: {returns.mean():.2f}%')\n",
    "            \n",
    "            # Statistics\n",
    "            skew = returns.skew()\n",
    "            kurt = returns.kurtosis()\n",
    "            \n",
    "            ax.set_title(f\"{description}\\nSkew: {skew:.2f}, Kurtosis: {kurt:.2f}\", fontsize=12)\n",
    "            ax.set_xlabel('Daily Return (%)')\n",
    "            ax.set_ylabel('Density')\n",
    "            ax.legend(loc='upper right', fontsize=8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nINTERPRETING SKEW AND KURTOSIS:\")\n",
    "print(\"-\" * 50)\n",
    "print(\"SKEWNESS (asymmetry):\")\n",
    "print(\"  • = 0: Symmetric\")\n",
    "print(\"  • < 0: Left tail longer (more crashes)\")\n",
    "print(\"  • > 0: Right tail longer (more spikes)\")\n",
    "print()\n",
    "print(\"KURTOSIS (tail thickness):\")\n",
    "print(\"  • = 0: Normal distribution tails\")\n",
    "print(\"  • > 0: Fatter tails (more extreme events than normal)\")\n",
    "print(\"  • < 0: Thinner tails (fewer extreme events)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Initial Event-Market Correlations\n",
    "\n",
    "### Correlation Analysis\n",
    "\n",
    "**Correlation** measures the linear relationship between two variables:\n",
    "\n",
    "- **r = +1**: Perfect positive correlation (move together)\n",
    "- **r = 0**: No linear correlation\n",
    "- **r = -1**: Perfect negative correlation (move opposite)\n",
    "\n",
    "**Important Caveat**: Correlation ≠ Causation!\n",
    "\n",
    "Just because events and returns are correlated doesn't mean events *cause* returns. There could be:\n",
    "- Reverse causation (market moves cause news coverage)\n",
    "- Confounding variables (something else causes both)\n",
    "- Coincidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# AGGREGATE EVENTS BY DATE\n",
    "# =============================================================================\n",
    "# We need to summarize multiple events per day into features\n",
    "# This is called \"feature engineering\" - creating useful inputs for analysis\n",
    "\n",
    "# Group events by date and calculate summary statistics\n",
    "daily_events = events_df.groupby('date').agg({\n",
    "    'goldstein_scale': ['mean', 'min', 'max', 'std', 'count'],\n",
    "    'num_mentions': 'sum',\n",
    "    'avg_tone': 'mean',\n",
    "}).reset_index()\n",
    "\n",
    "# Flatten MultiIndex columns\n",
    "# ('goldstein_scale', 'mean') becomes 'goldstein_mean'\n",
    "daily_events.columns = [\n",
    "    'date', \n",
    "    'goldstein_mean',   # Average event severity\n",
    "    'goldstein_min',    # Worst event of the day\n",
    "    'goldstein_max',    # Best event of the day\n",
    "    'goldstein_std',    # Spread of events (high = mixed day)\n",
    "    'event_count',      # Total number of events\n",
    "    'total_mentions',   # Total media coverage\n",
    "    'avg_tone'          # Average sentiment of coverage\n",
    "]\n",
    "\n",
    "# Fill NaN in std (happens when only one event per day)\n",
    "daily_events['goldstein_std'] = daily_events['goldstein_std'].fillna(0)\n",
    "\n",
    "print(\"Daily Event Features:\")\n",
    "print(\"=\" * 50)\n",
    "daily_events.describe().round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# MERGE EVENTS WITH MARKET DATA\n",
    "# =============================================================================\n",
    "# Join on date to create a single dataset\n",
    "\n",
    "if 'CL=F' in market_dfs:\n",
    "    oil_df = market_dfs['CL=F'].copy()\n",
    "    \n",
    "    # Left join: keep all market dates, fill missing events with NaN\n",
    "    merged = pd.merge(oil_df, daily_events, on='date', how='left')\n",
    "    \n",
    "    # Fill NaN event data with zeros (days with no events)\n",
    "    event_cols = ['goldstein_mean', 'goldstein_min', 'goldstein_max', \n",
    "                  'goldstein_std', 'event_count', 'total_mentions', 'avg_tone']\n",
    "    merged[event_cols] = merged[event_cols].fillna(0)\n",
    "    \n",
    "    # Remove rows with missing returns\n",
    "    merged = merged.dropna(subset=['log_return'])\n",
    "    \n",
    "    print(f\"Merged dataset: {len(merged)} rows\")\n",
    "    print(f\"Columns: {list(merged.columns)}\")\n",
    "    print()\n",
    "    merged.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CORRELATION MATRIX\n",
    "# =============================================================================\n",
    "# Calculate pairwise correlations between all numeric columns\n",
    "\n",
    "if 'merged' in dir() and len(merged) > 10:\n",
    "    # Select columns for correlation\n",
    "    corr_cols = ['log_return', 'goldstein_mean', 'goldstein_min', \n",
    "                 'event_count', 'total_mentions', 'avg_tone']\n",
    "    \n",
    "    # Calculate correlation matrix\n",
    "    corr_matrix = merged[corr_cols].corr()\n",
    "    \n",
    "    # Create heatmap\n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    \n",
    "    # Use diverging colormap centered at 0\n",
    "    sns.heatmap(corr_matrix, \n",
    "                annot=True,          # Show correlation values\n",
    "                fmt='.3f',           # 3 decimal places\n",
    "                cmap='RdYlGn',       # Red (negative) - Yellow (zero) - Green (positive)\n",
    "                center=0,            # Center colormap at 0\n",
    "                vmin=-1, vmax=1,     # Correlation ranges -1 to 1\n",
    "                square=True,         # Square cells\n",
    "                ax=ax)\n",
    "    \n",
    "    ax.set_title('Correlation Matrix: Oil Returns vs Event Features', fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Interpret correlations with returns\n",
    "    print(\"\\nCORRELATIONS WITH OIL RETURNS:\")\n",
    "    print(\"-\" * 50)\n",
    "    return_corrs = corr_matrix['log_return'].drop('log_return').sort_values(key=abs, ascending=False)\n",
    "    \n",
    "    for feature, corr in return_corrs.items():\n",
    "        strength = 'weak' if abs(corr) < 0.1 else 'moderate' if abs(corr) < 0.3 else 'strong'\n",
    "        direction = 'positive' if corr > 0 else 'negative'\n",
    "        print(f\"• {feature}: r = {corr:.3f} ({strength} {direction})\")\n",
    "else:\n",
    "    print(\"Insufficient data for correlation analysis.\")\n",
    "    print(\"Run the data ingestion scripts first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# SCATTER PLOTS\n",
    "# =============================================================================\n",
    "# Visualize relationships between events and returns\n",
    "\n",
    "if 'merged' in dir() and len(merged) > 10:\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    \n",
    "    # 1. Goldstein mean vs return\n",
    "    ax = axes[0, 0]\n",
    "    ax.scatter(merged['goldstein_mean'], merged['log_return'] * 100, \n",
    "               alpha=0.5, s=30, color='steelblue')\n",
    "    ax.axhline(y=0, color='black', linestyle='-', alpha=0.3)\n",
    "    ax.axvline(x=0, color='black', linestyle='-', alpha=0.3)\n",
    "    \n",
    "    # Add trend line\n",
    "    z = np.polyfit(merged['goldstein_mean'], merged['log_return'] * 100, 1)\n",
    "    p = np.poly1d(z)\n",
    "    x_line = np.linspace(merged['goldstein_mean'].min(), merged['goldstein_mean'].max(), 100)\n",
    "    ax.plot(x_line, p(x_line), 'r--', linewidth=2, label='Trend line')\n",
    "    \n",
    "    ax.set_xlabel('Average Goldstein Scale')\n",
    "    ax.set_ylabel('Oil Return (%)')\n",
    "    ax.set_title('Event Sentiment vs Oil Returns')\n",
    "    ax.legend()\n",
    "    \n",
    "    # 2. Event count vs absolute return\n",
    "    ax = axes[0, 1]\n",
    "    ax.scatter(merged['event_count'], merged['log_return'].abs() * 100,\n",
    "               alpha=0.5, s=30, color='orange')\n",
    "    ax.set_xlabel('Number of Events')\n",
    "    ax.set_ylabel('Absolute Oil Return (%)')\n",
    "    ax.set_title('Event Volume vs Return Magnitude')\n",
    "    \n",
    "    # 3. Worst event vs return\n",
    "    ax = axes[1, 0]\n",
    "    ax.scatter(merged['goldstein_min'], merged['log_return'] * 100,\n",
    "               alpha=0.5, s=30, color='red')\n",
    "    ax.axhline(y=0, color='black', linestyle='-', alpha=0.3)\n",
    "    ax.set_xlabel('Worst Event (min Goldstein)')\n",
    "    ax.set_ylabel('Oil Return (%)')\n",
    "    ax.set_title('Worst Daily Event vs Oil Returns')\n",
    "    \n",
    "    # 4. Media coverage vs absolute return\n",
    "    ax = axes[1, 1]\n",
    "    ax.scatter(merged['total_mentions'], merged['log_return'].abs() * 100,\n",
    "               alpha=0.5, s=30, color='purple')\n",
    "    ax.set_xlabel('Total Media Mentions')\n",
    "    ax.set_ylabel('Absolute Oil Return (%)')\n",
    "    ax.set_title('Media Coverage vs Return Magnitude')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nINTERPRETATION GUIDE:\")\n",
    "    print(\"-\" * 50)\n",
    "    print(\"• Scatter with clear trend → Potential predictive relationship\")\n",
    "    print(\"• Cloud with no pattern → Weak or no relationship\")\n",
    "    print(\"• Funnel shape → Relationship depends on another variable\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary & Next Steps\n",
    "\n",
    "### What We Learned\n",
    "\n",
    "| Topic | Key Findings |\n",
    "|-------|-------------|\n",
    "| **GDELT Events** | Goldstein scale measures conflict/cooperation; verbal events dominate |\n",
    "| **Market Data** | Returns are approximately normal with fat tails; VIX spikes during stress |\n",
    "| **Correlations** | Event features show weak-to-moderate correlation with returns |\n",
    "\n",
    "### Why Correlations Are Weak\n",
    "\n",
    "Don't be discouraged by low correlations! This is expected because:\n",
    "\n",
    "1. **Markets are efficient**: Information is priced in quickly\n",
    "2. **Many factors matter**: Events are just one input among many\n",
    "3. **Aggregation loses signal**: Daily averages mask individual event impacts\n",
    "4. **Timing matters**: Same-day analysis misses lead/lag relationships\n",
    "\n",
    "### Next Notebooks\n",
    "\n",
    "| Notebook | Analysis | Question Answered |\n",
    "|----------|----------|------------------|\n",
    "| `02_event_study_demo.ipynb` | Event Study | Did *this specific event* move the market? |\n",
    "| `03_anomaly_detection.ipynb` | Anomaly Detection | What unusual patterns exist in event-market relationships? |\n",
    "| `04_classification_demo.ipynb` | Classification | Can we predict market direction from events? |\n",
    "\n",
    "---\n",
    "\n",
    "### Key Data Science Skills Practiced\n",
    "\n",
    "- **Data Loading**: SQLAlchemy ORM → Pandas DataFrames\n",
    "- **Descriptive Statistics**: `.describe()`, `.value_counts()`\n",
    "- **Visualization**: Histograms, bar charts, scatter plots, heatmaps\n",
    "- **Feature Engineering**: Aggregating events by date\n",
    "- **Correlation Analysis**: Understanding relationships between variables"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
